{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1wUm162ncfKklQgO6AjO99iSMs95a9Uai",
      "authorship_tag": "ABX9TyPvR2TM8IqzcQSqYZ2LeWtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Floating-City/Final_Project_For_MATH-3790/blob/main/Word_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz8OVoCRRQsd",
        "outputId": "0a9825cb-002a-42c3-c177-3034d0c79acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O348Lgfxh4GY",
        "outputId": "e96e09c2-7a9d-41c0-e6e2-678a93ad649a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MarkovModelTagger:\n",
        "    def __init__(self):\n",
        "        self.model = defaultdict(Counter)\n",
        "\n",
        "    def train(self, tagged_sentences):\n",
        "        for sentence in tagged_sentences:\n",
        "            # Skip sentences with fewer than 3 tokens\n",
        "            if len(sentence) < 3:\n",
        "                continue\n",
        "            for ngram in ngrams(sentence, 3, pad_left=True, pad_right=True):\n",
        "                if None in ngram:\n",
        "                    continue\n",
        "                (w1, t1), (w2, t2), (w3, t3) = ngram\n",
        "                self.model[(t1, t2)][t3] += 1\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        tagged_sentence = []\n",
        "        prev_tags = (None, None)\n",
        "        for word in sentence:\n",
        "            if prev_tags in self.model and self.model[prev_tags]:\n",
        "                tag = self.model[prev_tags].most_common(1)[0][0]\n",
        "            elif (prev_tags[1],) in self.model and self.model[(prev_tags[1],)]:\n",
        "                tag = self.model[(prev_tags[1],)].most_common(1)[0][0]\n",
        "            elif word in self.model and self.model[word]:\n",
        "                tag = self.model[word].most_common(1)[0][0]\n",
        "            else:\n",
        "                tag = nltk.pos_tag([word])[0][1]  # Use NLTK's pos_tag as a fallback\n",
        "            tagged_sentence.append((word, tag))\n",
        "            prev_tags = (prev_tags[1], tag)\n",
        "        return tagged_sentence\n",
        "\n",
        "    def tag_sentence(self, sentence):\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        return self.tag(words)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('universal_dependencies', 'en_ewt', split='train')\n",
        "\n",
        "# Extract sentences\n",
        "sentences = [example['text'] for example in dataset]\n",
        "\n",
        "# Tokenize and tag sentences\n",
        "tagged_sentences = [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Initialize and train the tagger\n",
        "tagger = MarkovModelTagger()\n",
        "tagger.train(tagged_sentences)\n",
        "\n",
        "# Tag a sentence\n",
        "sentence = \"This is a sample test sentence\"\n",
        "print(tagger.tag_sentence(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo_q_jPohEjR",
        "outputId": "78e75ca2-752d-4998-d4f2-cad067278195"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NN'), ('test', 'IN'), ('sentence', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MarkovModelTagger:\n",
        "    def __init__(self):\n",
        "        self.model = defaultdict(Counter)\n",
        "\n",
        "    def train(self, tagged_sentences):\n",
        "        for sentence in tagged_sentences:\n",
        "            # Skip sentences with fewer than 3 tokens\n",
        "            if len(sentence) < 3:\n",
        "                continue\n",
        "            for ngram in ngrams(sentence, 3, pad_left=True, pad_right=True):\n",
        "                if None in ngram:\n",
        "                    continue\n",
        "                (w1, t1), (w2, t2), (w3, t3) = ngram\n",
        "                self.model[(t1, t2)][t3] += 1\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        tagged_sentence = []\n",
        "        prev_tags = (None, None)\n",
        "        for word in sentence:\n",
        "            if prev_tags in self.model and self.model[prev_tags]:\n",
        "                tag = self.model[prev_tags].most_common(1)[0][0]\n",
        "            elif (prev_tags[1],) in self.model and self.model[(prev_tags[1],)]:\n",
        "                tag = self.model[(prev_tags[1],)].most_common(1)[0][0]\n",
        "            elif word in self.model and self.model[word]:\n",
        "                tag = self.model[word].most_common(1)[0][0]\n",
        "            else:\n",
        "                tag = nltk.pos_tag([word])[0][1]  # Use NLTK's pos_tag as a fallback\n",
        "            tagged_sentence.append((word, tag))\n",
        "            prev_tags = (prev_tags[1], tag)\n",
        "        return tagged_sentence\n",
        "\n",
        "    def tag_sentence(self, sentence):\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        return self.tag(words)\n",
        "# Load the dataset\n",
        "dataset = load_dataset('universal_dependencies', 'en_ewt', split='train')\n",
        "\n",
        "sentences = [example['text'] for example in dataset]\n",
        "tagged_sentences = [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Initialize and train the tagger\n",
        "tagger = MarkovModelTagger()\n",
        "tagger.train(tagged_sentences)\n",
        "\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.tag import ClassifierBasedPOSTagger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.metrics.scores import precision, recall, f_measure\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset('universal_dependencies', 'en_ewt', split='train')\n",
        "\n",
        "sentences = [example['text'] for example in dataset]\n",
        "tagged_sentences = [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "train_set, test_set = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a new ClassifierBasedPOSTagger\n",
        "tagger = ClassifierBasedPOSTagger(train=train_set)\n",
        "\n",
        "# Test the tagger\n",
        "accuracy = tagger.evaluate(test_set)\n",
        "print(f\"ClassifierBasedPOSTagger accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Compare with MarkovModelTagger\n",
        "tagger_markov = MarkovModelTagger()\n",
        "tagger_markov.train(train_set)\n",
        "accuracy_markov = sum(t1 == t2 for sent in test_set for (w1, t1), (w2, t2) in zip(sent, tagger_markov.tag([w for w, t in sent]))) / sum(len(sent) for sent in test_set)\n",
        "print(f\"MarkovModelTagger accuracy: {accuracy_markov:.2f}\")\n",
        "\n",
        "# Perform a z-test\n",
        "z_score = (accuracy - accuracy_markov) / np.sqrt((accuracy * (1 - accuracy) / len(test_set)) + (accuracy_markov * (1 - accuracy_markov) / len(test_set)))\n",
        "print(f\"z-score: {z_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDNKo5pihfQM",
        "outputId": "e51cb36f-3ec4-45dc-fb1f-e481e0b7400d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5dd937de4052>:70: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = tagger.evaluate(test_set)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassifierBasedPOSTagger accuracy: 0.92\n",
            "MarkovModelTagger accuracy: 0.20\n",
            "z-score: 73.33\n"
          ]
        }
      ]
    }
  ]
}